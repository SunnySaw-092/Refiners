[mock_model1]
requires_grad = true
learning_rate = 1e-5

[mock_model2]
requires_grad = true

[clock]
verbose = false

[training]
duration = "100:epoch"
seed = 0
batch_size = 4
gradient_accumulation = 4
gradient_clipping_max_norm = 1.0

[optimizer]
optimizer = "SGD"
learning_rate = 1

[lr_scheduler]
type = "ConstantLR"
update_interval = "1:iteration"
